{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pretrained Transformer As Universal Computation Machine"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This assignment is made with the paper <a herf = \"https://arxiv.org/abs/2103.05247\" target = \"_blank\"> Pretrained Transformer As Universal Computation Machine</a>. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The transformer architecture has shown great success in deep learning, serving as backbone of larger model for tasks like NLP and images. Insipired by these successes, we aimed to explore the generalization capability of a transformer. We hypothesize transformers, if trained with on a data rich modality, such as a natural language corpus, can identify feture representations of **arbitrary* data. In this assignment, we invesigate whether pretrained language models are capable of in terms of generalizing to ther modalities with sequential structure."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To do this, we use a transformer model pretrained on natural language data: GPT-2 and only finetune the **linear input, linear ouput, positional embedding and layer norm** parameters. We will see how GPT-2 works in tasks completely different from language prediction. Then, we will show using pretrained transformer model as feature extractor has its advantage over building a new neural nets from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (4.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
            "Requirement already satisfied: filelock in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/changyiyang/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import random_split\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
        "from typing import List, Dict\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqhMY8cBy3FY"
      },
      "source": [
        "### **Learn about GPT2 architecture**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PljXgeYBzCA7"
      },
      "source": [
        "First, we introdcue the architecture of GPT2 and try build a sample model to simulate it to help you understand later parts better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-b32Pw7it8D"
      },
      "source": [
        "As you learned from lecture, the GPT2 is based on the transformer model, which is raised in paper, *Attention is all you need*. Here is a picture of transformer achitecture for your reference."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4FO4drgwzU9m"
      },
      "source": [
        "<div align=\"center\"><img src=https://jalammar.github.io/images/xlnet/transformer-encoder-decoder.png width=60% /> </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftdUrs66jG86"
      },
      "source": [
        "Then people find that only a stack of encoders/decoders are sufficent for taskes, which result in encoder-only transformer, such as BERT and decoder-only transformer, such as GPT2. Here are two pictures showes the architecture of encoder and decoder layer.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9GmXODWtjvpt"
      },
      "source": [
        "<div align=\"center\"><img src=https://jalammar.github.io/images/xlnet/transformer-encoder-block-2.png width=60% /> </div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\"><img src=https://jalammar.github.io/images/xlnet/transformer-decoder-block-2.png width=60% /> </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRyb__iNkQgQ"
      },
      "source": [
        "As you can see, the biggest difference between encoder and decoder is decoder has one more layer called **MASKED** self-attention. This layer is, as the name said, a self-attention layer with mask. To be specific, in masked self-attention, the causal relationshipd is added, which means for a certain word, only the words before it can have influence on it. The difference is well demostrated in the following pictures."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\"><img src=https://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png width=60% /> </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdUv3Q4QlIc2"
      },
      "source": [
        "Implementing the transformer or GPT2 is not the main focus of this homework. Here, we provided a detailed Pytorch implementation of a self-attention layer. You task is to apply the causual mask if `causal = True`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKizjZylqjWs"
      },
      "source": [
        "The scores is computed for you. The attention scores `scores[i, j]` represent the similarity score between the i-th query vector `q[i]` and the j-th key vector `k[j]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKMHBn4Cw-WJ"
      },
      "source": [
        "*Hint: `torch.triu` and `torch.masked_fill` function may be helpful.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5-k6YiVwfEA"
      },
      "outputs": [],
      "source": [
        "def dot_product_attention(q, k, v, causal=False):\n",
        "    \"\"\"\n",
        "    Computes the dot product attention scores and the attention output for a single example.\n",
        "\n",
        "    Args:\n",
        "    - q: Tensor of shape (query_length, embedding_size)\n",
        "    - k: Tensor of shape (key_length, embedding_size)\n",
        "    - v: Tensor of shape (value_length, embedding_size)\n",
        "    - causal: Boolean flag indicating whether to apply a causal mask\n",
        "\n",
        "    Returns:\n",
        "    - output: Tensor of shape (query_length, embedding_size)\n",
        "    \"\"\"\n",
        "    scores = torch.matmul(q, k.transpose(0, 1)) / (q.shape[-1] ** 0.5)  # shape: (query_length, key_length)\n",
        "    \n",
        "    if causal:\n",
        "\n",
        "      ############################################################################\n",
        "      # TODO: implement this part\n",
        "      ############################################################################\n",
        "        # Create a causal mask for the scores tensor\n",
        "        mask = torch.triu(torch.ones_like(scores), diagonal=1)\n",
        "        # print(mask)\n",
        "        scores.masked_fill_(mask == 1, float(\"-inf\"))\n",
        "        # print('scores',scores)\n",
        "\n",
        "      ############################################################################\n",
        "    \n",
        "    #print(scores)\n",
        "    weights = F.softmax(scores, dim=-1)  # shape: (query_length, key_length)\n",
        "    output = torch.matmul(weights, v)  # shape: (query_length, embedding_size)\n",
        "    \n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbQki1KQx9sy",
        "outputId": "d53aa10c-e165-459f-bb91-47a6be0e4dab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.],\n",
            "        [7., 8., 9.]])\n",
            "tensor([[6.9999, 7.9999, 8.9999],\n",
            "        [7.0000, 8.0000, 9.0000],\n",
            "        [7.0000, 8.0000, 9.0000]])\n"
          ]
        }
      ],
      "source": [
        "# test the implementation\n",
        "q = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
        "k = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
        "v = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
        "\n",
        "print(dot_product_attention(q, k, v, True))\n",
        "print(dot_product_attention(q, k, v, False))\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Application of GPT-2 in a Simple Language Task**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this part, we use GPT2 to do some language task, which is its original domain, to learn about how to adapt and fine tune GPT-2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # This part require finetuing\n",
        "# import torch\n",
        "# from transformers import GPT2Tokenizer, GPT2Config, GPT2ForSequenceClassification\n",
        "\n",
        "# # Load pre-trained model and tokenizer\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "# model_config = GPT2Config.from_pretrained('gpt2', num_labels=2)\n",
        "# model = GPT2ForSequenceClassification.from_pretrained(model_config)\n",
        "\n",
        "# # Define input text\n",
        "# input_text = \"This is an example input text for classification.\"\n",
        "# input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "# outputs = model(input_ids)\n",
        "\n",
        "# predicted_class = torch.argmax(outputs[0])\n",
        "\n",
        "# print(predicted_class)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Versatility of GPT-2 for tasks in other Domains**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this part, we will demonstrate how to adpat and finetune GPT-2 for tasks outside the language domain, including a math operation finding and image classification."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Task 1: Bit-wise operation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we show GPT-2 can well learn the bit-wise operation such as `AND`, `OR` and `XOR`. We will see it shows a extremely high accuracy on this task."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we need to create a dataset for fine tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "   device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we choose the `XOR` operator as our training goal. You are feel free to change the function below to test on other operators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# randomly generate two n-bits strings and its ground-truth and result\n",
        "def generate_example(n):\n",
        "  bits = np.random.randint(low=0, high=2, size=(2, n)) \n",
        "  \n",
        "  # change this line to change the operator\n",
        "  XOR = np.logical_xor(bits[0], bits[1]).astype(np.long) \n",
        "  # ----------------------------------------------------\n",
        "  \n",
        "  return bits.reshape((2*n)), XOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BitWiseDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, n, size):\n",
        "    self.n = n\n",
        "    self.size = size\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.size\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    bits = np.random.randint(low=0, high=2, size=(2, self.n))\n",
        "    And = np.logical_xor(bits[0], bits[1]).astype(np.int64)\n",
        "    return torch.tensor(bits.reshape((2*self.n)), dtype=torch.long).to(device), torch.tensor(And, dtype=torch.long).to(device)\n",
        "\n",
        "def generate_data_loaders(n, batch_size, data_size = 1000, train_size=0.8):\n",
        "  dataset = BitWiseDataset(n, size=data_size)\n",
        "  train_size = int(train_size * len(dataset))\n",
        "  test_size = len(dataset) - train_size\n",
        "  train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, test_loader\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, to fine tune on the GPT-2 model, we need to freeze the weights of the self-attention layer and feedforward layer. Only keep the layer norm layer and positional encnding changeable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the GP2 model\n",
        "gpt2 = GPT2Model.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wte.weight\n",
            "wpe.weight\n",
            "h.0.ln_1.weight\n",
            "h.0.ln_1.bias\n",
            "h.0.attn.c_attn.weight\n",
            "h.0.attn.c_attn.bias\n",
            "h.0.attn.c_proj.weight\n",
            "h.0.attn.c_proj.bias\n",
            "h.0.ln_2.weight\n",
            "h.0.ln_2.bias\n",
            "h.0.mlp.c_fc.weight\n",
            "h.0.mlp.c_fc.bias\n",
            "h.0.mlp.c_proj.weight\n",
            "h.0.mlp.c_proj.bias\n",
            "h.1.ln_1.weight\n",
            "h.1.ln_1.bias\n",
            "h.1.attn.c_attn.weight\n",
            "h.1.attn.c_attn.bias\n",
            "h.1.attn.c_proj.weight\n",
            "h.1.attn.c_proj.bias\n",
            "h.1.ln_2.weight\n",
            "h.1.ln_2.bias\n",
            "h.1.mlp.c_fc.weight\n",
            "h.1.mlp.c_fc.bias\n",
            "h.1.mlp.c_proj.weight\n",
            "h.1.mlp.c_proj.bias\n",
            "h.2.ln_1.weight\n",
            "h.2.ln_1.bias\n",
            "h.2.attn.c_attn.weight\n",
            "h.2.attn.c_attn.bias\n",
            "h.2.attn.c_proj.weight\n",
            "h.2.attn.c_proj.bias\n",
            "h.2.ln_2.weight\n",
            "h.2.ln_2.bias\n",
            "h.2.mlp.c_fc.weight\n",
            "h.2.mlp.c_fc.bias\n",
            "h.2.mlp.c_proj.weight\n",
            "h.2.mlp.c_proj.bias\n",
            "h.3.ln_1.weight\n",
            "h.3.ln_1.bias\n",
            "h.3.attn.c_attn.weight\n",
            "h.3.attn.c_attn.bias\n",
            "h.3.attn.c_proj.weight\n",
            "h.3.attn.c_proj.bias\n",
            "h.3.ln_2.weight\n",
            "h.3.ln_2.bias\n",
            "h.3.mlp.c_fc.weight\n",
            "h.3.mlp.c_fc.bias\n",
            "h.3.mlp.c_proj.weight\n",
            "h.3.mlp.c_proj.bias\n",
            "h.4.ln_1.weight\n",
            "h.4.ln_1.bias\n",
            "h.4.attn.c_attn.weight\n",
            "h.4.attn.c_attn.bias\n",
            "h.4.attn.c_proj.weight\n",
            "h.4.attn.c_proj.bias\n",
            "h.4.ln_2.weight\n",
            "h.4.ln_2.bias\n",
            "h.4.mlp.c_fc.weight\n",
            "h.4.mlp.c_fc.bias\n",
            "h.4.mlp.c_proj.weight\n",
            "h.4.mlp.c_proj.bias\n",
            "h.5.ln_1.weight\n",
            "h.5.ln_1.bias\n",
            "h.5.attn.c_attn.weight\n",
            "h.5.attn.c_attn.bias\n",
            "h.5.attn.c_proj.weight\n",
            "h.5.attn.c_proj.bias\n",
            "h.5.ln_2.weight\n",
            "h.5.ln_2.bias\n",
            "h.5.mlp.c_fc.weight\n",
            "h.5.mlp.c_fc.bias\n",
            "h.5.mlp.c_proj.weight\n",
            "h.5.mlp.c_proj.bias\n",
            "h.6.ln_1.weight\n",
            "h.6.ln_1.bias\n",
            "h.6.attn.c_attn.weight\n",
            "h.6.attn.c_attn.bias\n",
            "h.6.attn.c_proj.weight\n",
            "h.6.attn.c_proj.bias\n",
            "h.6.ln_2.weight\n",
            "h.6.ln_2.bias\n",
            "h.6.mlp.c_fc.weight\n",
            "h.6.mlp.c_fc.bias\n",
            "h.6.mlp.c_proj.weight\n",
            "h.6.mlp.c_proj.bias\n",
            "h.7.ln_1.weight\n",
            "h.7.ln_1.bias\n",
            "h.7.attn.c_attn.weight\n",
            "h.7.attn.c_attn.bias\n",
            "h.7.attn.c_proj.weight\n",
            "h.7.attn.c_proj.bias\n",
            "h.7.ln_2.weight\n",
            "h.7.ln_2.bias\n",
            "h.7.mlp.c_fc.weight\n",
            "h.7.mlp.c_fc.bias\n",
            "h.7.mlp.c_proj.weight\n",
            "h.7.mlp.c_proj.bias\n",
            "h.8.ln_1.weight\n",
            "h.8.ln_1.bias\n",
            "h.8.attn.c_attn.weight\n",
            "h.8.attn.c_attn.bias\n",
            "h.8.attn.c_proj.weight\n",
            "h.8.attn.c_proj.bias\n",
            "h.8.ln_2.weight\n",
            "h.8.ln_2.bias\n",
            "h.8.mlp.c_fc.weight\n",
            "h.8.mlp.c_fc.bias\n",
            "h.8.mlp.c_proj.weight\n",
            "h.8.mlp.c_proj.bias\n",
            "h.9.ln_1.weight\n",
            "h.9.ln_1.bias\n",
            "h.9.attn.c_attn.weight\n",
            "h.9.attn.c_attn.bias\n",
            "h.9.attn.c_proj.weight\n",
            "h.9.attn.c_proj.bias\n",
            "h.9.ln_2.weight\n",
            "h.9.ln_2.bias\n",
            "h.9.mlp.c_fc.weight\n",
            "h.9.mlp.c_fc.bias\n",
            "h.9.mlp.c_proj.weight\n",
            "h.9.mlp.c_proj.bias\n",
            "h.10.ln_1.weight\n",
            "h.10.ln_1.bias\n",
            "h.10.attn.c_attn.weight\n",
            "h.10.attn.c_attn.bias\n",
            "h.10.attn.c_proj.weight\n",
            "h.10.attn.c_proj.bias\n",
            "h.10.ln_2.weight\n",
            "h.10.ln_2.bias\n",
            "h.10.mlp.c_fc.weight\n",
            "h.10.mlp.c_fc.bias\n",
            "h.10.mlp.c_proj.weight\n",
            "h.10.mlp.c_proj.bias\n",
            "h.11.ln_1.weight\n",
            "h.11.ln_1.bias\n",
            "h.11.attn.c_attn.weight\n",
            "h.11.attn.c_attn.bias\n",
            "h.11.attn.c_proj.weight\n",
            "h.11.attn.c_proj.bias\n",
            "h.11.ln_2.weight\n",
            "h.11.ln_2.bias\n",
            "h.11.mlp.c_fc.weight\n",
            "h.11.mlp.c_fc.bias\n",
            "h.11.mlp.c_proj.weight\n",
            "h.11.mlp.c_proj.bias\n",
            "ln_f.weight\n",
            "ln_f.bias\n"
          ]
        }
      ],
      "source": [
        "# show the name of all the para your are able to modified in this model\n",
        "for name, param in gpt2.named_parameters():\n",
        "  print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for name, param in gpt2.named_parameters():\n",
        "# freeze all parameters except the layernorm and positional embeddings \n",
        "  if 'ln' in name or 'wpe' in name:\n",
        "    param.requires_grad = True \n",
        "  else:\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we have our dataset and pretrained model ready, we need to adapt the model to our task, which means adding a embedding layer before the model and a linear output layer after the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Bit_wise_transformer(nn.Module):\n",
        "  def __init__(self, engine, bitLength, input_dim, engine_embed_dim, n_class = 2):\n",
        "    super().__init__()\n",
        "    self.n = bitLength\n",
        "    self.input_embed = nn.Embedding(input_dim, engine_embed_dim)\n",
        "    self.engine = engine\n",
        "    self.output_layer = nn.Linear(engine_embed_dim, n_class)\n",
        "  def forward(self, x):\n",
        "    embeddings = self.input_embed(x)\n",
        "    hidden_state = self.engine(inputs_embeds=embeddings).last_hidden_state[:,self.n:]\n",
        "    logits = self.output_layer(hidden_state)[0]\n",
        "    return logits"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we are ready for the training! Generate the train and set set first, and train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate the training and testing data\n",
        "train_loader, test_loader = generate_data_loaders(n=5, batch_size=1,data_size = 1000, train_size=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create an instance of the model\n",
        "Bit_length = 5\n",
        "\n",
        "model = Bit_wise_transformer(\n",
        "      gpt2, \n",
        "      bitLength = Bit_length,\n",
        "      input_dim = 2, \n",
        "      engine_embed_dim = 768\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# define the optimizer and loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "losses = []\n",
        "train_acc = []\n",
        "all_val_acc = []\n",
        "best_val_acc = 0\n",
        "num_epochs = 5\n",
        "\n",
        "epoch_iterator = range(num_epochs)\n",
        "for epoch in epoch_iterator:\n",
        "    # Training loop\n",
        "    running_loss = 0.0\n",
        "    data_iterator = tqdm(train_loader)\n",
        "    for i, (inputs, labels) in enumerate(data_iterator):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.squeeze())\n",
        "        accuracy = torch.mean((torch.argmax(outputs, dim=-1) == labels.flatten()).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        data_iterator.set_postfix(loss=loss.item())\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i + 1) % 10 == 0:\n",
        "              #print(f'Epoch [{epoch + 1}/{num_epochs}], Data [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
        "            running_loss = 0.0\n",
        "            losses.append(loss.item())\n",
        "            train_acc.append(accuracy.item())\n",
        "        \n",
        "    # Validation\n",
        "    val_acc = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in test_loader:\n",
        "          inputs = inputs.to(device=device, dtype=torch.long)\n",
        "          labels = labels.to(device=device, dtype=torch.long)\n",
        "          outputs = model(inputs)\n",
        "          accuracy = torch.mean((torch.argmax(outputs, dim=-1) == labels.flatten()).float())\n",
        "          val_acc.append(accuracy.item())\n",
        "    model.train()\n",
        "\n",
        "    all_val_acc.append(np.mean(val_acc))\n",
        "    # Save best model\n",
        "    if np.mean(val_acc) > best_val_acc:\n",
        "        best_val_acc = np.mean(val_acc)\n",
        "\n",
        "    epoch_iterator.set_postfix(val_acc=np.mean(val_acc), best_val_acc=best_val_acc)\n",
        "\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.title('Train Loss')\n",
        "plt.figure()\n",
        "plt.plot(train_acc)\n",
        "plt.title('Train Accuracy')\n",
        "plt.figure()\n",
        "plt.plot(all_val_acc)\n",
        "plt.title('Val Accuracy')\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, in the final plots of training and testing accuracy, you should see GPT-2 achieve a extremely good result."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Task 2: Image classfication"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For sure the last task is quite simple and may not so convincing to show the generality of GPT-2. Now we move to a more complex domain, image classfication. We will do the task on the famous hand-written dataset MNIST and reach a good result"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Still, we need to freeze some parameters in GPT-2 first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A function freeze necessary parameters in GPT-2\n",
        "def gpt2_freezer(\n",
        "    model,                         \n",
        "    freeze_param_list: List[str] = None\n",
        "):\n",
        "  for (name, param) in model.named_parameters():\n",
        "    if freeze_param_list is not None:\n",
        "      if any([k in name for k in freeze_param_list]):\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "      param.requires_grad = False\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pretrained_gpt2 = GPT2Model.from_pretrained('gpt2') \n",
        "gpt2_engine = gpt2_freezer(\n",
        "    pretrained_gpt2,\n",
        "    [\"mlp\", \"attn\"]\n",
        "  )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Advantages of Pre-trained Models: Speed and Accuracy**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Influence of Model Capacity on Accuracy and Training Time**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Interpreting Attention Layers in GPT-2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "83d8af4ff1ccf0e43da2803a684c950e5c1874edeab0ee20d27ee23b5d3b3a9c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
