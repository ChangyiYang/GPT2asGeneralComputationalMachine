{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_lf8zgUaD-U"
      },
      "source": [
        "# Versatility of GPT-2 for Tasks in other Domains:\n",
        "\n",
        "tasks: Bit XOR, Bit Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnzsuAJSZtGf",
        "outputId": "7c037b54-feb6-491d-cdb8-042ca4c6d1df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.0\n"
          ]
        }
      ],
      "source": [
        "#@title Install Package\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roteYkBlZ5N-"
      },
      "outputs": [],
      "source": [
        "#@title Import Package\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
        "from typing import List, Dict\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "%matplotlib inline "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ6F-G_-H4sc"
      },
      "source": [
        "## Task 1: Bit-wise operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHQTVab1IH6Q"
      },
      "source": [
        "### part A: Creating the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_j4JaYRQFGx"
      },
      "source": [
        "Simple logic of the generate the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NZ4iGEzI_hj"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "   device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T88qShTIHQw"
      },
      "outputs": [],
      "source": [
        "# randomly generate two n-bits strings and its ground-truth and result\n",
        "def generate_example(n):\n",
        "  bits = np.random.randint(low=0, high=2, size=(2, n)) \n",
        "  XOR = np.logical_xor(bits[0], bits[1]).astype(np.long) \n",
        "  return bits.reshape((2*n)), XOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnejtUozIbec",
        "outputId": "a45f6dc3-f77b-428f-f908-57b5577d8417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  String 1: [0 1 1 0 1]\n",
            "  String 2: [1 0 1 0 0]\n",
            "Output AND: [1 1 0 0 1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-92986c992621>:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  And = np.logical_xor(bits[0], bits[1]).astype(np.long)\n"
          ]
        }
      ],
      "source": [
        "n=5\n",
        "bits, XOR = generate_example(n)\n",
        "print('  String 1:', bits[:n])\n",
        "print('  String 2:', bits[n:])\n",
        "print('Output XOR:', XOR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE5-R0WrQLJe"
      },
      "source": [
        "Create the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKbTZvOQQOVL"
      },
      "outputs": [],
      "source": [
        "class BitWiseDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, n, size):\n",
        "    self.n = n\n",
        "    self.size = size\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.size\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    bits = np.random.randint(low=0, high=2, size=(2, self.n))\n",
        "    And = np.logical_xor(bits[0], bits[1]).astype(np.int64)\n",
        "    return torch.tensor(bits.reshape((2*self.n)), dtype=torch.long).to(device), torch.tensor(And, dtype=torch.long).to(device)\n",
        "\n",
        "def generate_data_loaders(n, batch_size, data_size = 1000, train_size=0.8):\n",
        "  dataset = BitWiseDataset(n, size=data_size)\n",
        "  train_size = int(train_size * len(dataset))\n",
        "  test_size = len(dataset) - train_size\n",
        "  train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r48UAsImI_yT"
      },
      "source": [
        "### part B: Modify and train the pretained transformer to our specific tasks\n",
        "Load the pretain transformer GPT2-model and freeze the weights of the self-attention and feedforward layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "d8d8aa2a34ea4bd8ac14afbaffad0cfb",
            "84c3c279ab424de69e56ca775517ef4c",
            "b2e0db3bbe9c4f0391a823f34223d29b",
            "cd4cfb5634f14b32ab038a192c673f63",
            "78c06fe8d2ed495992079f685217312c",
            "0f3b58d8f7a049ec8193594a4ff729ef",
            "e675c9e1981d4e92bc070c647c14b23a",
            "e172bdff284c492988e983c06097a776",
            "81f45fb72bc74b7f8a5913dfecacd779",
            "5bb5d1de9b7f48a491f8849a37acfe43",
            "b89e334a89e6466c87dfaa7d6d94c668",
            "ee92bed1b29b4d52b40610b48261ca80",
            "4e37b83d3ee0427c8721955fb2ac6422",
            "1484567fc2924dc1b98968f6c0347a90",
            "53bb4f7f40f2493ab3da30084130e266",
            "cac1bd49e7c84a76b7939ac22ee09220",
            "85912f0fa5044f59a0d9fcaee98bafaf",
            "4f94e4b0de214c338d1f7fc1a146833d",
            "72b7aed5ec4a416bac813cffb2cd9a89",
            "cb8b375d99d343afa8c2bbd655cdc231",
            "06778664e47e4ebc9edb70d6ed692dd7",
            "7e8d56988cb14e23a3d298b57ec2cc2c"
          ]
        },
        "id": "CVlROyu1KWRi",
        "outputId": "8333d663-5b82-4ee6-c03a-c0a1d46b3111"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8d8aa2a34ea4bd8ac14afbaffad0cfb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee92bed1b29b4d52b40610b48261ca80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# load the GP2 model\n",
        "gpt2 = GPT2Model.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZLesJq7LJfZ",
        "outputId": "4efcf24d-e950-4b13-d3f0-7f11172bb415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wte.weight\n",
            "wpe.weight\n",
            "h.0.ln_1.weight\n",
            "h.0.ln_1.bias\n",
            "h.0.attn.c_attn.weight\n",
            "h.0.attn.c_attn.bias\n",
            "h.0.attn.c_proj.weight\n",
            "h.0.attn.c_proj.bias\n",
            "h.0.ln_2.weight\n",
            "h.0.ln_2.bias\n",
            "h.0.mlp.c_fc.weight\n",
            "h.0.mlp.c_fc.bias\n",
            "h.0.mlp.c_proj.weight\n",
            "h.0.mlp.c_proj.bias\n",
            "h.1.ln_1.weight\n",
            "h.1.ln_1.bias\n",
            "h.1.attn.c_attn.weight\n",
            "h.1.attn.c_attn.bias\n",
            "h.1.attn.c_proj.weight\n",
            "h.1.attn.c_proj.bias\n",
            "h.1.ln_2.weight\n",
            "h.1.ln_2.bias\n",
            "h.1.mlp.c_fc.weight\n",
            "h.1.mlp.c_fc.bias\n",
            "h.1.mlp.c_proj.weight\n",
            "h.1.mlp.c_proj.bias\n",
            "h.2.ln_1.weight\n",
            "h.2.ln_1.bias\n",
            "h.2.attn.c_attn.weight\n",
            "h.2.attn.c_attn.bias\n",
            "h.2.attn.c_proj.weight\n",
            "h.2.attn.c_proj.bias\n",
            "h.2.ln_2.weight\n",
            "h.2.ln_2.bias\n",
            "h.2.mlp.c_fc.weight\n",
            "h.2.mlp.c_fc.bias\n",
            "h.2.mlp.c_proj.weight\n",
            "h.2.mlp.c_proj.bias\n",
            "h.3.ln_1.weight\n",
            "h.3.ln_1.bias\n",
            "h.3.attn.c_attn.weight\n",
            "h.3.attn.c_attn.bias\n",
            "h.3.attn.c_proj.weight\n",
            "h.3.attn.c_proj.bias\n",
            "h.3.ln_2.weight\n",
            "h.3.ln_2.bias\n",
            "h.3.mlp.c_fc.weight\n",
            "h.3.mlp.c_fc.bias\n",
            "h.3.mlp.c_proj.weight\n",
            "h.3.mlp.c_proj.bias\n",
            "h.4.ln_1.weight\n",
            "h.4.ln_1.bias\n",
            "h.4.attn.c_attn.weight\n",
            "h.4.attn.c_attn.bias\n",
            "h.4.attn.c_proj.weight\n",
            "h.4.attn.c_proj.bias\n",
            "h.4.ln_2.weight\n",
            "h.4.ln_2.bias\n",
            "h.4.mlp.c_fc.weight\n",
            "h.4.mlp.c_fc.bias\n",
            "h.4.mlp.c_proj.weight\n",
            "h.4.mlp.c_proj.bias\n",
            "h.5.ln_1.weight\n",
            "h.5.ln_1.bias\n",
            "h.5.attn.c_attn.weight\n",
            "h.5.attn.c_attn.bias\n",
            "h.5.attn.c_proj.weight\n",
            "h.5.attn.c_proj.bias\n",
            "h.5.ln_2.weight\n",
            "h.5.ln_2.bias\n",
            "h.5.mlp.c_fc.weight\n",
            "h.5.mlp.c_fc.bias\n",
            "h.5.mlp.c_proj.weight\n",
            "h.5.mlp.c_proj.bias\n",
            "h.6.ln_1.weight\n",
            "h.6.ln_1.bias\n",
            "h.6.attn.c_attn.weight\n",
            "h.6.attn.c_attn.bias\n",
            "h.6.attn.c_proj.weight\n",
            "h.6.attn.c_proj.bias\n",
            "h.6.ln_2.weight\n",
            "h.6.ln_2.bias\n",
            "h.6.mlp.c_fc.weight\n",
            "h.6.mlp.c_fc.bias\n",
            "h.6.mlp.c_proj.weight\n",
            "h.6.mlp.c_proj.bias\n",
            "h.7.ln_1.weight\n",
            "h.7.ln_1.bias\n",
            "h.7.attn.c_attn.weight\n",
            "h.7.attn.c_attn.bias\n",
            "h.7.attn.c_proj.weight\n",
            "h.7.attn.c_proj.bias\n",
            "h.7.ln_2.weight\n",
            "h.7.ln_2.bias\n",
            "h.7.mlp.c_fc.weight\n",
            "h.7.mlp.c_fc.bias\n",
            "h.7.mlp.c_proj.weight\n",
            "h.7.mlp.c_proj.bias\n",
            "h.8.ln_1.weight\n",
            "h.8.ln_1.bias\n",
            "h.8.attn.c_attn.weight\n",
            "h.8.attn.c_attn.bias\n",
            "h.8.attn.c_proj.weight\n",
            "h.8.attn.c_proj.bias\n",
            "h.8.ln_2.weight\n",
            "h.8.ln_2.bias\n",
            "h.8.mlp.c_fc.weight\n",
            "h.8.mlp.c_fc.bias\n",
            "h.8.mlp.c_proj.weight\n",
            "h.8.mlp.c_proj.bias\n",
            "h.9.ln_1.weight\n",
            "h.9.ln_1.bias\n",
            "h.9.attn.c_attn.weight\n",
            "h.9.attn.c_attn.bias\n",
            "h.9.attn.c_proj.weight\n",
            "h.9.attn.c_proj.bias\n",
            "h.9.ln_2.weight\n",
            "h.9.ln_2.bias\n",
            "h.9.mlp.c_fc.weight\n",
            "h.9.mlp.c_fc.bias\n",
            "h.9.mlp.c_proj.weight\n",
            "h.9.mlp.c_proj.bias\n",
            "h.10.ln_1.weight\n",
            "h.10.ln_1.bias\n",
            "h.10.attn.c_attn.weight\n",
            "h.10.attn.c_attn.bias\n",
            "h.10.attn.c_proj.weight\n",
            "h.10.attn.c_proj.bias\n",
            "h.10.ln_2.weight\n",
            "h.10.ln_2.bias\n",
            "h.10.mlp.c_fc.weight\n",
            "h.10.mlp.c_fc.bias\n",
            "h.10.mlp.c_proj.weight\n",
            "h.10.mlp.c_proj.bias\n",
            "h.11.ln_1.weight\n",
            "h.11.ln_1.bias\n",
            "h.11.attn.c_attn.weight\n",
            "h.11.attn.c_attn.bias\n",
            "h.11.attn.c_proj.weight\n",
            "h.11.attn.c_proj.bias\n",
            "h.11.ln_2.weight\n",
            "h.11.ln_2.bias\n",
            "h.11.mlp.c_fc.weight\n",
            "h.11.mlp.c_fc.bias\n",
            "h.11.mlp.c_proj.weight\n",
            "h.11.mlp.c_proj.bias\n",
            "ln_f.weight\n",
            "ln_f.bias\n"
          ]
        }
      ],
      "source": [
        "# show the name of all the para your are able to modified in this model\n",
        "for name, param in gpt2.named_parameters():\n",
        "  print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqVLEIzVK_MO"
      },
      "outputs": [],
      "source": [
        "for name, param in gpt2.named_parameters():\n",
        "# freeze all parameters except the layernorm and positional embeddings \n",
        "  if 'ln' in name or 'wpe' in name:\n",
        "    param.requires_grad = True \n",
        "  else:\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDjkXJeGKAB7"
      },
      "source": [
        "Design and train your own network using pretrained GPT2 model as universal computational engine. You need to design\n",
        "\n",
        "*   An input embedding layer, which projects your input to accommodate GPT2 input demension\n",
        "\n",
        "*   An output Linear layer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgPK3_sJJ_uB"
      },
      "outputs": [],
      "source": [
        "class Bit_wise_transformer(nn.Module):\n",
        "  def __init__(self, engine, bitLength, input_dim, engine_embed_dim, n_class = 2):\n",
        "    super().__init__()\n",
        "    self.n = bitLength\n",
        "    self.input_embed = nn.Embedding(input_dim, engine_embed_dim)\n",
        "    self.engine = engine\n",
        "    self.output_layer = nn.Linear(engine_embed_dim, n_class)\n",
        "  def forward(self, x):\n",
        "    embeddings = self.input_embed(x)\n",
        "    hidden_state = self.engine(inputs_embeds=embeddings).last_hidden_state[:,self.n:]\n",
        "    logits = self.output_layer(hidden_state)[0]\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5Nlkb1p_Nv_"
      },
      "outputs": [],
      "source": [
        "# generate the training and testing data\n",
        "train_loader, test_loader = generate_data_loaders(n=5, batch_size=1,data_size = 1000, train_size=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzG0yfZPKAZZ",
        "outputId": "c9d25b38-a28c-476b-da50-990a4c12caac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-e37a48aed99d>:11: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  And = np.logical_and(bits[0], bits[1]).astype(np.long)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Batch [100/800], Loss: 0.3933\n",
            "Epoch [1/5], Batch [200/800], Loss: 0.3741\n",
            "Epoch [1/5], Batch [300/800], Loss: 0.3219\n",
            "Epoch [1/5], Batch [400/800], Loss: 0.3138\n",
            "Epoch [1/5], Batch [500/800], Loss: 0.2900\n",
            "Epoch [1/5], Batch [600/800], Loss: 0.2794\n",
            "Epoch [1/5], Batch [700/800], Loss: 0.2708\n",
            "Epoch [1/5], Batch [800/800], Loss: 0.2949\n",
            "Epoch [2/5], Batch [100/800], Loss: 0.2990\n",
            "Epoch [2/5], Batch [200/800], Loss: 0.2838\n",
            "Epoch [2/5], Batch [300/800], Loss: 0.2754\n",
            "Epoch [2/5], Batch [400/800], Loss: 0.2564\n",
            "Epoch [2/5], Batch [500/800], Loss: 0.2650\n",
            "Epoch [2/5], Batch [600/800], Loss: 0.2348\n",
            "Epoch [2/5], Batch [700/800], Loss: 0.2304\n",
            "Epoch [2/5], Batch [800/800], Loss: 0.2433\n",
            "Epoch [3/5], Batch [100/800], Loss: 0.2076\n",
            "Epoch [3/5], Batch [200/800], Loss: 0.2196\n",
            "Epoch [3/5], Batch [300/800], Loss: 0.2113\n",
            "Epoch [3/5], Batch [400/800], Loss: 0.2883\n",
            "Epoch [3/5], Batch [500/800], Loss: 0.2538\n",
            "Epoch [3/5], Batch [600/800], Loss: 0.2374\n",
            "Epoch [3/5], Batch [700/800], Loss: 0.2331\n",
            "Epoch [3/5], Batch [800/800], Loss: 0.2474\n",
            "Epoch [4/5], Batch [100/800], Loss: 0.3064\n",
            "Epoch [4/5], Batch [200/800], Loss: 0.2762\n",
            "Epoch [4/5], Batch [300/800], Loss: 0.2697\n",
            "Epoch [4/5], Batch [400/800], Loss: 0.2597\n",
            "Epoch [4/5], Batch [500/800], Loss: 0.2656\n",
            "Epoch [4/5], Batch [600/800], Loss: 0.2209\n",
            "Epoch [4/5], Batch [700/800], Loss: 0.2351\n",
            "Epoch [4/5], Batch [800/800], Loss: 0.2239\n",
            "Epoch [5/5], Batch [100/800], Loss: 0.2172\n",
            "Epoch [5/5], Batch [200/800], Loss: 0.1917\n",
            "Epoch [5/5], Batch [300/800], Loss: 0.1866\n",
            "Epoch [5/5], Batch [400/800], Loss: 0.1636\n",
            "Epoch [5/5], Batch [500/800], Loss: 0.1405\n",
            "Epoch [5/5], Batch [600/800], Loss: 0.1171\n",
            "Epoch [5/5], Batch [700/800], Loss: 0.0778\n",
            "Epoch [5/5], Batch [800/800], Loss: 0.1191\n"
          ]
        }
      ],
      "source": [
        "#@title train the modified model\n",
        "# create an instance of the model\n",
        "model = Bit_wise_transformer(\n",
        "      gpt2, \n",
        "      bitLength = Bit_length,\n",
        "      input_dim = 2, \n",
        "      engine_embed_dim = 768\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# define the optimizer and loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "losses = []\n",
        "train_acc = []\n",
        "all_val_acc = []\n",
        "best_val_acc = 0\n",
        "num_epochs = 5\n",
        "\n",
        "epoch_iterator = trange(num_epochs)\n",
        "for epoch in epoch_iterator:\n",
        "    # Training loop\n",
        "    running_loss = 0.0\n",
        "    data_iterator = tqdm(train_loader)\n",
        "    for i, (inputs, labels) in enumerate(data_iterator):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.squeeze())\n",
        "        accuracy = torch.mean((torch.argmax(outputs, dim=-1) == labels.flatten()).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        data_iterator.set_postfix(loss=loss.item())\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i + 1) % 10 == 0:\n",
        "              #print(f'Epoch [{epoch + 1}/{num_epochs}], Data [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
        "            running_loss = 0.0\n",
        "            losses.append(loss.item())\n",
        "            train_acc.append(accuracy.item())\n",
        "        \n",
        "    # Validation\n",
        "    val_acc = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in test_loader:\n",
        "          inputs = inputs.to(device=device, dtype=torch.long)\n",
        "          labels = labels.to(device=device, dtype=torch.long)\n",
        "          outputs = model(inputs)\n",
        "          accuracy = torch.mean((torch.argmax(outputs, dim=-1) == labels.flatten()).float())\n",
        "          val_acc.append(accuracy.item())\n",
        "    model.train()\n",
        "\n",
        "    all_val_acc.append(np.mean(val_acc))\n",
        "    # Save best model\n",
        "    if np.mean(val_acc) > best_val_acc:\n",
        "        best_val_acc = np.mean(val_acc)\n",
        "\n",
        "    epoch_iterator.set_postfix(val_acc=np.mean(val_acc), best_val_acc=best_val_acc)\n",
        "\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.title('Train Loss')\n",
        "plt.figure()\n",
        "plt.plot(train_acc)\n",
        "plt.title('Train Accuracy')\n",
        "plt.figure()\n",
        "plt.plot(all_val_acc)\n",
        "plt.title('Val Accuracy')\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwn5mOUQt-_o"
      },
      "source": [
        "## Task 2: Bit Memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deuBSg-vt-_x"
      },
      "source": [
        "### part A: Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEoFqTQFt-_x"
      },
      "source": [
        "Simple logic of the generate the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p_OJElRt-_x"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "   device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ost2FoMdt-_x"
      },
      "source": [
        "Create the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb844P10zHvS"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "\n",
        "    def __init__(self, device='cpu'):\n",
        "        self.device = device\n",
        "        self._ind = 0\n",
        "\n",
        "    def get_batch(self, batch_size, train=True):\n",
        "        x, y = self.get_batch_np(batch_size, train=train)\n",
        "        x = torch.from_numpy(x).to(device=self.device, dtype=torch.float32)\n",
        "        y = torch.from_numpy(y).to(device=self.device, dtype=torch.long)\n",
        "        self._ind += 1\n",
        "        return x, y\n",
        "\n",
        "    def get_batch_np(self, batch_size, train):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def start_epoch(self):\n",
        "        self._ind = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OonY5LnzOYb"
      },
      "outputs": [],
      "source": [
        "class BitMemoryDataset(Dataset):\n",
        "\n",
        "    def __init__(self, n=1000, num_patterns=5, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.n = n\n",
        "        self.num_patterns = num_patterns\n",
        "\n",
        "    def get_batch_np(self, batch_size, train):\n",
        "        bits = np.random.randint(low=0, high=2, size=(batch_size, self.num_patterns, self.n))\n",
        "        bits = 2 * bits - 1\n",
        "        query_inds = np.random.randint(low=0, high=self.num_patterns, size=batch_size)\n",
        "        query_bits = bits[range(batch_size), query_inds]\n",
        "        mask = np.random.randint(low=0, high=2, size=query_bits.shape)\n",
        "        masked_query_bits = mask * query_bits\n",
        "        masked_query_bits = masked_query_bits.reshape(batch_size, 1, self.n)\n",
        "        x = np.concatenate([bits, masked_query_bits], axis=1)\n",
        "        y = query_bits\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtbAaSu8Blaa"
      },
      "outputs": [],
      "source": [
        "class BitMemoryDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,bit_length =100, num_patterns=5, size = 100):\n",
        "    self.bit_length = bit_length\n",
        "    self.num_patterns = num_patterns\n",
        "    self.size = size\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.size\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    bits = np.random.randint(low=0, high=2, size=(1, self.num_patterns, self.bit_length))\n",
        "    bits = 2 * bits - 1\n",
        "    query_inds = np.random.randint(low=0, high=self.num_patterns, size=1)\n",
        "    query_bits = bits[range(1), query_inds]\n",
        "    mask = np.random.randint(low=1, high=2, size=query_bits.shape)\n",
        "    masked_query_bits = mask * query_bits\n",
        "    masked_query_bits = masked_query_bits.reshape(1, 1, self.bit_length)\n",
        "    x = np.concatenate([bits, masked_query_bits], axis=1)\n",
        "    y = query_bits\n",
        "\n",
        "    return torch.tensor(x, dtype=torch.float32).squeeze(), torch.tensor(y, dtype=torch.long).squeeze()\n",
        "\n",
        "\n",
        "\n",
        "def generate_data_loaders(size = 100, batch_size = 1, train_size=0.8):\n",
        "  dataset = BitMemoryDataset(size=size)\n",
        "  train_size = int(train_size * len(dataset))\n",
        "  test_size = len(dataset) - train_size\n",
        "  train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5FTLlHGt-_y"
      },
      "source": [
        "### part B: Modified the pretained transformer to our specific tasks\n",
        "Load the pretain transformer GPT2-model and freeze the weights of the self-attention and feedforward layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoGys8jnt-_y"
      },
      "outputs": [],
      "source": [
        "# load the GP2 model\n",
        "gpt2 = GPT2Model.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82z3QLunt-_y",
        "outputId": "a3931692-23b3-4275-88f9-cbdcc93bdd63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wte.weight\n",
            "wpe.weight\n",
            "h.0.ln_1.weight\n",
            "h.0.ln_1.bias\n",
            "h.0.attn.c_attn.weight\n",
            "h.0.attn.c_attn.bias\n",
            "h.0.attn.c_proj.weight\n",
            "h.0.attn.c_proj.bias\n",
            "h.0.ln_2.weight\n",
            "h.0.ln_2.bias\n",
            "h.0.mlp.c_fc.weight\n",
            "h.0.mlp.c_fc.bias\n",
            "h.0.mlp.c_proj.weight\n",
            "h.0.mlp.c_proj.bias\n",
            "h.1.ln_1.weight\n",
            "h.1.ln_1.bias\n",
            "h.1.attn.c_attn.weight\n",
            "h.1.attn.c_attn.bias\n",
            "h.1.attn.c_proj.weight\n",
            "h.1.attn.c_proj.bias\n",
            "h.1.ln_2.weight\n",
            "h.1.ln_2.bias\n",
            "h.1.mlp.c_fc.weight\n",
            "h.1.mlp.c_fc.bias\n",
            "h.1.mlp.c_proj.weight\n",
            "h.1.mlp.c_proj.bias\n",
            "h.2.ln_1.weight\n",
            "h.2.ln_1.bias\n",
            "h.2.attn.c_attn.weight\n",
            "h.2.attn.c_attn.bias\n",
            "h.2.attn.c_proj.weight\n",
            "h.2.attn.c_proj.bias\n",
            "h.2.ln_2.weight\n",
            "h.2.ln_2.bias\n",
            "h.2.mlp.c_fc.weight\n",
            "h.2.mlp.c_fc.bias\n",
            "h.2.mlp.c_proj.weight\n",
            "h.2.mlp.c_proj.bias\n",
            "h.3.ln_1.weight\n",
            "h.3.ln_1.bias\n",
            "h.3.attn.c_attn.weight\n",
            "h.3.attn.c_attn.bias\n",
            "h.3.attn.c_proj.weight\n",
            "h.3.attn.c_proj.bias\n",
            "h.3.ln_2.weight\n",
            "h.3.ln_2.bias\n",
            "h.3.mlp.c_fc.weight\n",
            "h.3.mlp.c_fc.bias\n",
            "h.3.mlp.c_proj.weight\n",
            "h.3.mlp.c_proj.bias\n",
            "h.4.ln_1.weight\n",
            "h.4.ln_1.bias\n",
            "h.4.attn.c_attn.weight\n",
            "h.4.attn.c_attn.bias\n",
            "h.4.attn.c_proj.weight\n",
            "h.4.attn.c_proj.bias\n",
            "h.4.ln_2.weight\n",
            "h.4.ln_2.bias\n",
            "h.4.mlp.c_fc.weight\n",
            "h.4.mlp.c_fc.bias\n",
            "h.4.mlp.c_proj.weight\n",
            "h.4.mlp.c_proj.bias\n",
            "h.5.ln_1.weight\n",
            "h.5.ln_1.bias\n",
            "h.5.attn.c_attn.weight\n",
            "h.5.attn.c_attn.bias\n",
            "h.5.attn.c_proj.weight\n",
            "h.5.attn.c_proj.bias\n",
            "h.5.ln_2.weight\n",
            "h.5.ln_2.bias\n",
            "h.5.mlp.c_fc.weight\n",
            "h.5.mlp.c_fc.bias\n",
            "h.5.mlp.c_proj.weight\n",
            "h.5.mlp.c_proj.bias\n",
            "h.6.ln_1.weight\n",
            "h.6.ln_1.bias\n",
            "h.6.attn.c_attn.weight\n",
            "h.6.attn.c_attn.bias\n",
            "h.6.attn.c_proj.weight\n",
            "h.6.attn.c_proj.bias\n",
            "h.6.ln_2.weight\n",
            "h.6.ln_2.bias\n",
            "h.6.mlp.c_fc.weight\n",
            "h.6.mlp.c_fc.bias\n",
            "h.6.mlp.c_proj.weight\n",
            "h.6.mlp.c_proj.bias\n",
            "h.7.ln_1.weight\n",
            "h.7.ln_1.bias\n",
            "h.7.attn.c_attn.weight\n",
            "h.7.attn.c_attn.bias\n",
            "h.7.attn.c_proj.weight\n",
            "h.7.attn.c_proj.bias\n",
            "h.7.ln_2.weight\n",
            "h.7.ln_2.bias\n",
            "h.7.mlp.c_fc.weight\n",
            "h.7.mlp.c_fc.bias\n",
            "h.7.mlp.c_proj.weight\n",
            "h.7.mlp.c_proj.bias\n",
            "h.8.ln_1.weight\n",
            "h.8.ln_1.bias\n",
            "h.8.attn.c_attn.weight\n",
            "h.8.attn.c_attn.bias\n",
            "h.8.attn.c_proj.weight\n",
            "h.8.attn.c_proj.bias\n",
            "h.8.ln_2.weight\n",
            "h.8.ln_2.bias\n",
            "h.8.mlp.c_fc.weight\n",
            "h.8.mlp.c_fc.bias\n",
            "h.8.mlp.c_proj.weight\n",
            "h.8.mlp.c_proj.bias\n",
            "h.9.ln_1.weight\n",
            "h.9.ln_1.bias\n",
            "h.9.attn.c_attn.weight\n",
            "h.9.attn.c_attn.bias\n",
            "h.9.attn.c_proj.weight\n",
            "h.9.attn.c_proj.bias\n",
            "h.9.ln_2.weight\n",
            "h.9.ln_2.bias\n",
            "h.9.mlp.c_fc.weight\n",
            "h.9.mlp.c_fc.bias\n",
            "h.9.mlp.c_proj.weight\n",
            "h.9.mlp.c_proj.bias\n",
            "h.10.ln_1.weight\n",
            "h.10.ln_1.bias\n",
            "h.10.attn.c_attn.weight\n",
            "h.10.attn.c_attn.bias\n",
            "h.10.attn.c_proj.weight\n",
            "h.10.attn.c_proj.bias\n",
            "h.10.ln_2.weight\n",
            "h.10.ln_2.bias\n",
            "h.10.mlp.c_fc.weight\n",
            "h.10.mlp.c_fc.bias\n",
            "h.10.mlp.c_proj.weight\n",
            "h.10.mlp.c_proj.bias\n",
            "h.11.ln_1.weight\n",
            "h.11.ln_1.bias\n",
            "h.11.attn.c_attn.weight\n",
            "h.11.attn.c_attn.bias\n",
            "h.11.attn.c_proj.weight\n",
            "h.11.attn.c_proj.bias\n",
            "h.11.ln_2.weight\n",
            "h.11.ln_2.bias\n",
            "h.11.mlp.c_fc.weight\n",
            "h.11.mlp.c_fc.bias\n",
            "h.11.mlp.c_proj.weight\n",
            "h.11.mlp.c_proj.bias\n",
            "ln_f.weight\n",
            "ln_f.bias\n"
          ]
        }
      ],
      "source": [
        "# show the name of all the para your are able to modified in this model\n",
        "for name, param in gpt2.named_parameters():\n",
        "  print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEV6wZA-t-_y"
      },
      "outputs": [],
      "source": [
        "for name, param in gpt2.named_parameters():\n",
        "# freeze all parameters except the layernorm and positional embeddings \n",
        "  if 'ln' in name or 'wpe' in name:\n",
        "    param.requires_grad = True \n",
        "  else:\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bExq8Swlt-_y"
      },
      "source": [
        "Design and train your own network using pretrained GPT2 model as universal computational engine. You need to design\n",
        "\n",
        "*   An input embedding layer, which projects your input to accommodate GPT2 input demension\n",
        "\n",
        "*   An output Linear layer and loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MddjdKz-t-_y"
      },
      "outputs": [],
      "source": [
        "class Bit_Memory_transformer(nn.Module):\n",
        "  def __init__(self, engine,input_dim, engine_embed_dim, n_class, patch_size):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = n_class\n",
        "    self.input_embed = nn.Linear(input_dim, engine_embed_dim)\n",
        "    self.engine = engine\n",
        "    self.output_layer = nn.Linear(engine_embed_dim, n_class)\n",
        "  def forward(self, x):\n",
        "        orig_dim = x.shape[-1]\n",
        "        ratio = orig_dim // self.input_dim\n",
        "        x = x.reshape(x.shape[0], x.shape[1] * ratio, self.input_dim)\n",
        "        flattened_input = x.view(-1, 50)\n",
        "\n",
        "        x = self.input_embed(flattened_input)\n",
        "        \n",
        "        x = self.engine(inputs_embeds = x.unsqueeze(0)).last_hidden_state\n",
        "        x = x[:,-ratio:]\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        x = x.reshape(x.shape[0], x.shape[1] // ratio, ratio * self.output_dim)\n",
        "        return x\n",
        "        \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1HVV80V-4Lq"
      },
      "outputs": [],
      "source": [
        "ce_loss = torch.nn.CrossEntropyLoss()\n",
        "task = 'bit-memory'\n",
        "n = 100\n",
        "def loss_fn(out, y, x=None):\n",
        "  out = torch.reshape(out, (-1, n, 2))\n",
        "  ids = torch.zeros(y.shape).to(device=y.device).long()\n",
        "  if task == 'bit-memory':\n",
        "      ids[y < 0], ids[y > 0] = 0, 1\n",
        "  else:\n",
        "      ids[y < 0.5], ids[y > 0.5] = 0, 1\n",
        "  out, ids = torch.reshape(out, (-1, 2)), torch.reshape(ids, (-1,))\n",
        "  return ce_loss(out, ids)\n",
        "\n",
        "def accuracy_fn(preds, true, x=None):\n",
        "    if task == 'bit-memory':\n",
        "        preds = preds.reshape(-1, n, 2).argmax(-1) * 2 - 1\n",
        "    else:\n",
        "        preds = preds.reshape(-1, n, 2).argmax(-1)\n",
        "    if task == 'bit-memory':\n",
        "        return (np.sign(preds) == np.sign(true)).mean()\n",
        "    else:\n",
        "        return ((preds > 0.5) == (true > 0.5)).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSYHy0JZF9yQ",
        "outputId": "551cafbb-3256-4104-b450-30746ec043e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/3], Batch [100/800], Loss: 1.7978\n",
            "Epoch [1/3], Batch [200/800], Loss: 1.8176\n",
            "Epoch [1/3], Batch [300/800], Loss: 1.8573\n",
            "Epoch [1/3], Batch [400/800], Loss: 1.7865\n",
            "Epoch [1/3], Batch [500/800], Loss: 1.8767\n",
            "Epoch [1/3], Batch [600/800], Loss: 1.9043\n",
            "Epoch [1/3], Batch [700/800], Loss: 1.8139\n",
            "Epoch [1/3], Batch [800/800], Loss: 1.8507\n",
            "Epoch [2/3], Batch [100/800], Loss: 1.8376\n",
            "Epoch [2/3], Batch [200/800], Loss: 1.8109\n",
            "Epoch [2/3], Batch [300/800], Loss: 1.8670\n",
            "Epoch [2/3], Batch [400/800], Loss: 1.8446\n",
            "Epoch [2/3], Batch [500/800], Loss: 1.8444\n",
            "Epoch [2/3], Batch [600/800], Loss: 1.8390\n",
            "Epoch [2/3], Batch [700/800], Loss: 1.8304\n",
            "Epoch [2/3], Batch [800/800], Loss: 1.8627\n",
            "Epoch [3/3], Batch [100/800], Loss: 1.8082\n",
            "Epoch [3/3], Batch [200/800], Loss: 1.8063\n",
            "Epoch [3/3], Batch [300/800], Loss: 1.8468\n",
            "Epoch [3/3], Batch [400/800], Loss: 1.8781\n",
            "Epoch [3/3], Batch [500/800], Loss: 1.8946\n",
            "Epoch [3/3], Batch [600/800], Loss: 1.8022\n",
            "Epoch [3/3], Batch [700/800], Loss: 1.8479\n",
            "Epoch [3/3], Batch [800/800], Loss: 1.8332\n"
          ]
        }
      ],
      "source": [
        "model = Bit_Memory_transformer(\n",
        "    gpt2, \n",
        "    input_dim = 50, \n",
        "    engine_embed_dim = 768,\n",
        "    n_class= 2*50,\n",
        "    patch_size = 50\n",
        ")\n",
        "\n",
        "def get_loss(x,y,return_acc = True):\n",
        "  out = model(x)\n",
        "  loss = loss_fn(out, y, x=x)\n",
        "  accs = accuracy_fn(\n",
        "                out.detach().cpu().numpy(),\n",
        "                y.detach().cpu().numpy(),\n",
        "                x=x.detach().cpu().numpy(),\n",
        "            )\n",
        "  return loss, accs\n",
        "\n",
        "train_loader, test_loader = generate_data_loaders(size = 1000, batch_size=1, train_size=0.8)\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    # Training loop\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "      loss, acc = get_loss(inputs, labels, return_acc=True)\n",
        "      loss.backward()\n",
        "      running_loss += loss.detach().cpu().item()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      if (i + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
        "            running_loss = 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtzGsEILIqRa"
      },
      "outputs": [],
      "source": [
        "   # Testing loop\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        loss, acc = get_loss(inputs, labels, return_acc=True)\n",
        "        test_loss += loss.detach().cpu().item() / len(test_loader)\n",
        "        accuracy += acc / len(test_loader)\n",
        "\n",
        "print(len(test_loader))\n",
        "print(f'Testing accuracy: {100 * accuracy} %')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06778664e47e4ebc9edb70d6ed692dd7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f3b58d8f7a049ec8193594a4ff729ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1484567fc2924dc1b98968f6c0347a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72b7aed5ec4a416bac813cffb2cd9a89",
            "max": 548118077,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb8b375d99d343afa8c2bbd655cdc231",
            "value": 548118077
          }
        },
        "4e37b83d3ee0427c8721955fb2ac6422": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85912f0fa5044f59a0d9fcaee98bafaf",
            "placeholder": "​",
            "style": "IPY_MODEL_4f94e4b0de214c338d1f7fc1a146833d",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "4f94e4b0de214c338d1f7fc1a146833d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53bb4f7f40f2493ab3da30084130e266": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06778664e47e4ebc9edb70d6ed692dd7",
            "placeholder": "​",
            "style": "IPY_MODEL_7e8d56988cb14e23a3d298b57ec2cc2c",
            "value": " 548M/548M [00:05&lt;00:00, 111MB/s]"
          }
        },
        "5bb5d1de9b7f48a491f8849a37acfe43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72b7aed5ec4a416bac813cffb2cd9a89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78c06fe8d2ed495992079f685217312c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e8d56988cb14e23a3d298b57ec2cc2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81f45fb72bc74b7f8a5913dfecacd779": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84c3c279ab424de69e56ca775517ef4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f3b58d8f7a049ec8193594a4ff729ef",
            "placeholder": "​",
            "style": "IPY_MODEL_e675c9e1981d4e92bc070c647c14b23a",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "85912f0fa5044f59a0d9fcaee98bafaf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2e0db3bbe9c4f0391a823f34223d29b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e172bdff284c492988e983c06097a776",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81f45fb72bc74b7f8a5913dfecacd779",
            "value": 665
          }
        },
        "b89e334a89e6466c87dfaa7d6d94c668": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cac1bd49e7c84a76b7939ac22ee09220": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb8b375d99d343afa8c2bbd655cdc231": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd4cfb5634f14b32ab038a192c673f63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bb5d1de9b7f48a491f8849a37acfe43",
            "placeholder": "​",
            "style": "IPY_MODEL_b89e334a89e6466c87dfaa7d6d94c668",
            "value": " 665/665 [00:00&lt;00:00, 17.7kB/s]"
          }
        },
        "d8d8aa2a34ea4bd8ac14afbaffad0cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84c3c279ab424de69e56ca775517ef4c",
              "IPY_MODEL_b2e0db3bbe9c4f0391a823f34223d29b",
              "IPY_MODEL_cd4cfb5634f14b32ab038a192c673f63"
            ],
            "layout": "IPY_MODEL_78c06fe8d2ed495992079f685217312c"
          }
        },
        "e172bdff284c492988e983c06097a776": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e675c9e1981d4e92bc070c647c14b23a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee92bed1b29b4d52b40610b48261ca80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e37b83d3ee0427c8721955fb2ac6422",
              "IPY_MODEL_1484567fc2924dc1b98968f6c0347a90",
              "IPY_MODEL_53bb4f7f40f2493ab3da30084130e266"
            ],
            "layout": "IPY_MODEL_cac1bd49e7c84a76b7939ac22ee09220"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
